{
  "metadata": {
    "name": "Streaming Queries with Apache Flink",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Week 3 Lab 2: Streaming Queries with Apache Flink\n\nWelcome to Apache Zeppelin. Zeppeling is a web-based multi-purpose notebook that enables interactive data analysis and visualization. It supports multiple languages including Python, Scala, SQL, and more. In Zeppelin there is a concept named interpreters. An interpreter allows any language or data-processing-backend to be plugged into Zeppelin. Interpreters in Zeppelin are essentially components that allow the execution of code written in different languages or for different data processing engines within the same notebook environment. Each interpreter is associated with a specific language or processing engine and is prefixed with a special syntax (e.g., %python, %spark, %sql) to indicate the type of code to be executed. For example, this cell uses the markdown interpreter `%md`. During this lab you will use Apache Flink to process streaming data; in particular, you will use [PyFlink](https://nightlies.apache.org/flink/flink-docs-master/api/python/). PyFlink is the Python API for Apache Flink, allowing to write Flink applications in Python. On the other hand, Apache Flink is an open-source stream processing framework that enables scalable, high-throughput, low-latency data stream and batch processing.\n\n\nIn this notebook you will interact with PyFlink through the interpreter `%flink.pyflink`; you will see this command at the beginning of each of the following cells. First, will import some necessary packages from PyFlink."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\nimport os\nimport json\n\nfrom datetime import datetime\nfrom pyflink.common import Row\nfrom pyflink.table.expressions import col, lit\nfrom pyflink.table import (EnvironmentSettings, StreamTableEnvironment, TableEnvironment, TableDescriptor, Schema,\n                           DataTypes, FormatDescriptor)\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table.window import Slide\nfrom pyflink.table.udf import udf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "During this lab you will use the PyFilnk\u0027s [Table API](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/python/table/intro_to_table_api/). The Table API is a high-level API for batch and stream processing of data that is part of Apache Flink. It provides a SQL-like interface to define and execute data processing pipelines. The Table API allows you to work with structured data in a declarative manner, making it easier to write complex data transformations and aggregations. There is another API in PyFlink named the DataStream API, but it is beyond the scope of this lab. The DataStream API is a low-level API designed for complex event-driven applications and stream processing. If you want to know more about this API, you can always \ncheck the [documentation](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/python/datastream/intro_to_datastream_api/#create-using-table--sql-connectors)\n\n\nIn the Table API there are some important concepts:\n* `TableEnvironment` is the central concept of the Table API in PyFlink, it is responsible of manage tables and metadata, configuration, and execution of table operations.\n* A `Table` represents a logical set of rows with a schema. Tables are the primary abstraction for performing data transformations. It does not contain the data itself in any way. Instead, it describes how to read data from a table source, and how to eventually write data to a table sink.\n* The `Expressions` are used to define operations on columns in a Table. \n\nFirst, set up the environment for executing table programs in streaming mode with the following lines:"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\nenv_settings \u003d EnvironmentSettings.in_streaming_mode()\ntable_env \u003d TableEnvironment.create(env_settings)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nThe first line of the previous cell creates an `EnvironmentSettings` object configured for streaming mode store at the variable `env_settings`. `EnvironmentSetting`s is used to specify the characteristics of the execution environment for Flink’s Table API.\n\nThe second line creates a `TableEnvironment` based on the settings provided in the `env_settings` and stores it in `table_env`. Now, you will use the table environment object just created to develop the tables and queries for your streaming dataset. First, you will create a table named `source_stream`. Remember that tables do not contain the data itself, but help you to describe how to read the data from the source streaming. For that, you will create a table with the schema of the dataset that arrives from your input streaming. As an example, the data that you will ingest looks like this:\n\n```json\n{\n    \u0027order_id\u0027: \u00271f4da8b2-73d0-49d5-9762-3e2e0a3cf004\u0027, \n    \u0027order_timestamp\u0027: \u00272024-04-04T15:32:03\u0027, \n    \u0027order_date\u0027: \u00272024-04-04\u0027, \n    \u0027customer_number\u0027: 198, \n    \u0027customer_visit_number\u0027: 1,\n    \u0027customer_city\u0027: \u0027Makati City\u0027, \n    \u0027customer_country\u0027: \u0027Philippines\u0027,\n    \u0027customer_credit_limit\u0027: 60237,\n    \u0027device_type\u0027: \u0027desktop\u0027, \n    \u0027browser\u0027: \u0027Opera/9.32.(Windows 98; tig-ER) Presto/2.9.179 Version/10.00\u0027, \n    \u0027operating_system\u0027: \u0027Android\u0027, \n    \u0027product_code\u0027: \u0027S32_1268\u0027, \n    \u0027product_line\u0027: \u0027Trucks and Buses\u0027,\n    \u0027product_unitary_price\u0027: 96.31, \n    \u0027quantity\u0027: 10, \n    \u0027total_price\u0027: 963.1,\n    \u0027traffic_source\u0027: \u0027www.hardin-green.com\u0027\n}\n```\n\nIdentify the [data type](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/python/table/python_types/) of each field and see how the table will be created below. Note that you can set the `order_timestamp` field as `TIMESTAMP(0)`; this data type means the timestamp will have zero fractional digits in the seconds field, effectively truncating any millisecond, microsecond, or nanosecond parts. This means it represents a precise date and time down to the second."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\ntable_env.execute_sql(\"DROP TABLE IF EXISTS source_stream;\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\ninput_table_name \u003d \"source_stream\"\ntable_env.execute_sql(f\"DROP TABLE IF EXISTS {input_table_name};\")\n\ninput_stream_name \u003d \"de-c3w3lab2-kinesis-input-stream\"\n\nregion\u003d\"us-east-1\"\nsource_table_ddl \u003d \"\"\"\n  CREATE TABLE {0} (\n    order_id STRING,\n    order_timestamp TIMESTAMP(0),\n    order_date STRING,\n    customer_number INT,\n    customer_visit_number INT,\n    customer_city STRING,\n    customer_country STRING,\n    customer_credit_limit INT,\n    device_type STRING,\n    browser STRING,\n    operating_system STRING,\n    product_code STRING, \n    product_line STRING,\n    product_unitary_price NUMERIC,\n    quantity INT, \n    total_price NUMERIC,\n    traffic_source STRING,\n    WATERMARK FOR order_timestamp AS order_timestamp - INTERVAL \u00275\u0027 MINUTES)\n    PARTITIONED BY (order_id)\n    WITH (\n    \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n    \u0027stream\u0027 \u003d \u0027{1}\u0027,\n    \u0027aws.region\u0027 \u003d \u0027{2}\u0027,\n    \u0027format\u0027 \u003d \u0027json\u0027,\n    \u0027scan.stream.initpos\u0027 \u003d \u0027TRIM_HORIZON\u0027,\n    \u0027json.timestamp-format.standard\u0027 \u003d \u0027ISO-8601\u0027\n    ) \"\"\".format(input_table_name, input_stream_name, region)\ntable_env.execute_sql(source_table_ddl)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In the previous script you were already given with part of the creation query. You can see the following parts:\n* `WATERMARK` command, which is used in stream processing to handle late data and out-of-order events. Watermarks indicate the progress of event time in a stream. \n    * In the Watermark, the `FOR` clause specifies that the watermark is associated with the `order_timestamp` column.\n    * Furthermore, the `AS order_timestamp - INTERVAL \u00275\u0027 MINUTES` clause defines that the system expects events to arrive within 5 minutes of their event time. Events arriving later than 5 minutes are considered late.\n* `WITH` clause specifies the table\u0027s connector and other properties essential to define how Flink connects to the datasource. Between the most relevant ones you can find:\n    * `\u0027connector\u0027 \u003d \u0027kinesis\u0027` specifies that the table will read from an AWS Kinesis stream.\n    * `\u0027format\u0027 \u003d \u0027json\u0027` specifies that the data format of the stream is JSON.\n    * `\u0027scan.stream.initpos\u0027 \u003d \u0027TRIM_HORIZON\u0027`. This specifies the initial position in the stream from which to start reading. `\u0027TRIM_HORIZON`\u0027 means it will start reading from the earliest available record in the stream.\n    * `\u0027json.timestamp-format.standard\u0027 \u003d \u0027ISO-8601\u0027` specifies that the timestamp format in the JSON data is ISO-8601.\n\nIf you want to know more about the connectors, you can check the [documentation](https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/kinesis/).\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Now, let\u0027s create an User Defined Function (UDF) to convert the timestamps into string. This is helpful as a workaround to save timestamps in AWS Kinesis.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n@udf(input_types\u003d[DataTypes.TIMESTAMP(3)], result_type\u003dDataTypes.STRING())\ndef to_string(i):\n    return str(i)\ntable_env.create_temporary_system_function(\"to_string\", to_string)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nNow, you will create a sliding window table from your `source_stream`. Here you can use the sliding window query that you created when working with the local data sample to get the total number of sales. \nIn this case, define the window size to be of 6 minutes while the window slide to be of 3 minutes."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n# Sliding window query\ninput_table \u003d table_env.from_path(\"source_stream\")\nsliding_window_table \u003d (\n        input_table.window(\n            Slide.over(lit(6).minute)\n            .every(lit(3).minutes)\n            .on(col(\"order_timestamp\"))\n            .alias(\"six_minute_window\")\n        )\n        .group_by(col(\"six_minute_window\"))\n        .select(to_string(col(\"six_minute_window\").end).alias(\"event_time\"), col(\"total_price\").sum.alias(\"total_sales\"))\n    )"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Create a temporary view in your table environment based on the sliding window table that you created.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\ntable_env.create_temporary_view(\"sliding_window_table\", sliding_window_table)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nIn the next cell, you have to define the schema of your sink table, which will be actually one of the output AWS Kinesis data streams that have been created for you. You can set the `event_time` field as a string by simplicity. Use the `de-c3w3lab2-kinesis-total-sales-slide-output-stream` output stream and name your output table as `\"output_sliding_sales_stream\"`"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\ntable_name \u003d \"output_sliding_sales_stream\"\ntable_env.execute_sql(f\"DROP TABLE IF EXISTS {table_name};\")\n\nstream_name \u003d \"de-c3w3lab2-kinesis-total-sales-slide-output-stream\"\n\nregion\u003d\"us-east-1\"\nsource_table_ddl \u003d \"\"\"\n CREATE TABLE {0} (\n    event_time STRING,\n    total_sales NUMERIC)\n\n    WITH (\n    \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n    \u0027stream\u0027 \u003d \u0027{1}\u0027,\n    \u0027aws.region\u0027 \u003d \u0027{2}\u0027,\n    \u0027format\u0027 \u003d \u0027json\u0027,\n    \u0027json.timestamp-format.standard\u0027 \u003d \u0027ISO-8601\u0027\n    ) \"\"\".format(table_name, stream_name, region)\ntable_env.execute_sql(source_table_ddl)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nAfter you have defined the schema in your output table, insert the data from the sliding window table into your sink:"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\ntable_result \u003d table_env.execute_sql(\"INSERT INTO {0} SELECT * FROM {1}\"\n                                     .format(\"output_sliding_sales_stream\", \"sliding_window_table\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nAfter you execute the previous cell, you will see in the upper right side of that cell that appeared an option to open a `Flink Job`. Click on it and another tab will be open where you can interact with the Flink GUI. \nThis interface shows the state of the Flink jobs and will even allow you to take a look to the data. \n\nIn this lab, you will use the consumer provided to you. Go to VSCode and run the following commands to run the consumer script:\n\n```bash\npython3 scripts/consumer/src/consumer.py de-c3w3lab2-kinesis-total-sales-slide-output-stream\n```\n\nYou should start seeing some processed records with the schema that you just generated and should look similar to this output:\n\n```json\n{\u0027event_time\u0027: \u00272024-06-01 01:00:00\u0027, \u0027total_sales\u0027: 25385}\n```"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Great! You have created your first sliding window query over a datastream! The next step for you is to create another sliding window query that allows you to get the number of orders per device type. For that, follow these instructions:\n* Create an `input_table` from your `\"source_stream\"` table.\n* From `input_table` create a window with a size of 6 minutes and a slide of 3 minutes on the `order_timestamp` column. Name this column as ``six_minute_window`.\n* Then, group by the columns: `six_minute_window` and `device_type`. Remember to use the `col` function to reference columns in the table.\n* Then, select the `six_minute_window` column, cast it as string and save it as `event_time`. Select also your `device_type` column and count `order_id`; save it as `orders_count`\n\nSave your query table to `orders_by_device_window_table`."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\ninput_table \u003d table_env.from_path(\"source_stream\")\n\norders_by_device_window_table \u003d (\n    input_table.window(\n        Slide.over(lit(6).minute)\n        .every(lit(3).minutes)\n        .on(col(\"order_timestamp\"))\n        .alias(\"six_minute_window\")\n    )\n    .group_by(col(\"six_minute_window\"), col(\"device_type\"))\n    .select(\n        to_string(col(\"six_minute_window\").end).alias(\"event_time\"),\n        col(\"device_type\"),\n        col(\"order_id\").count.alias(\"orders_count\")\n    )\n)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Create the temporary view from your output table. Name it as `orders_by_device_window_table`"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\ntable_env.create_temporary_view(\"orders_by_device_window_table\", orders_by_device_window_table)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Now, create the schema of your output: \n* The table name should be `output_device_sliding_stream`\n* Your schema should include the following columns: `event_time` as string, `device_type` also as string, and `orders_count` as a numeric value.\n* Remember to set your connector to `kinesis`.\n* Your output stream name is `de-c3w3lab2-kinesis-devices-output-stream`."
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\ntable_name \u003d \"output_device_sliding_stream\"\ntable_env.execute_sql(f\"DROP TABLE IF EXISTS {table_name};\")\n\nstream_name \u003d \"de-c3w3lab2-kinesis-devices-output-stream\"\n\nregion\u003d\"us-east-1\"\nsource_table_ddl \u003d \"\"\"\n CREATE TABLE {0} (\n    event_time STRING,\n    device_type STRING,\n    orders_count NUMERIC)\n\n    WITH (\n    \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n    \u0027stream\u0027 \u003d \u0027{1}\u0027,\n    \u0027aws.region\u0027 \u003d \u0027{2}\u0027,\n    \u0027format\u0027 \u003d \u0027json\u0027,\n    \u0027json.timestamp-format.standard\u0027 \u003d \u0027ISO-8601\u0027\n    ) \"\"\".format(table_name, stream_name, region)\ntable_env.execute_sql(source_table_ddl)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "With your output schema created, start inserting data from your sliding window table into your output table. Remember to open the `Flink job` once it is available and to run your consumer from VSCode terminal with\n\n```bash\npython3 scripts/consumer/src/consumer.py  de-c3w3lab2-kinesis-devices-output-stream\n```\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\ntable_result \u003d table_env.execute_sql(\"INSERT INTO {0} SELECT * FROM {1}\"\n                                     .format(\"output_device_sliding_stream\", \"orders_by_device_window_table\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In this lab, you explored the capabilities of PyFlink within a Zeppelin notebook to perform advanced stream processing tasks. By creating sliding window and tumbled window queries, you gained hands-on experience with two fundamental types of window operations used in stream processing:\n* Sliding Windows: You learned how to create sliding window queries that allow for overlapping time intervals, providing a continuous and detailed view of your data over time. This is useful for real-time monitoring and detecting trends.\n* Tumbled Windows: You implemented tumbled window queries, which divide the stream into non-overlapping, fixed-size windows. This is ideal for periodic reporting and aggregating data over uniform intervals.\n \nYou applied windowing functions to simulate real-time analytics scenarios, such as summarizing data over time windows. By working with event time attributes and defining watermarks, you ensured accurate and timely processing of out-of-order events, which is critical for reliable stream processing.\n\nThe ability to process and analyze streaming data in real time is a valuable asset in today\u0027s data-driven world, opening up opportunities for innovation and insights in various domains.\n"
    }
  ]
}